{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('sentiwordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "NEGATE = {\n",
    "    \"n't\",\n",
    "    \"aint\",\n",
    "    \"arent\",\n",
    "    \"cannot\",\n",
    "    \"cant\",\n",
    "    \"couldnt\",\n",
    "    \"darent\",\n",
    "    \"didnt\",\n",
    "    \"doesnt\",\n",
    "    \"ain't\",\n",
    "    \"aren't\",\n",
    "    \"can't\",\n",
    "    \"couldn't\",\n",
    "    \"daren't\",\n",
    "    \"didn't\",\n",
    "    \"doesn't\",\n",
    "    \"dont\",\n",
    "    \"hadnt\",\n",
    "    \"hasnt\",\n",
    "    \"havent\",\n",
    "    \"isnt\",\n",
    "    \"mightnt\",\n",
    "    \"mustnt\",\n",
    "    \"neither\",\n",
    "    \"don't\",\n",
    "    \"hadn't\",\n",
    "    \"hasn't\",\n",
    "    \"haven't\",\n",
    "    \"isn't\",\n",
    "    \"mightn't\",\n",
    "    \"mustn't\",\n",
    "    \"neednt\",\n",
    "    \"needn't\",\n",
    "    \"never\",\n",
    "    \"none\",\n",
    "    \"nope\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"nowhere\",\n",
    "    \"oughtnt\",\n",
    "    \"shant\",\n",
    "    \"shouldnt\",\n",
    "    \"uhuh\",\n",
    "    \"wasnt\",\n",
    "    \"werent\",\n",
    "    \"oughtn't\",\n",
    "    \"shan't\",\n",
    "    \"shouldn't\",\n",
    "    \"uh-uh\",\n",
    "    \"wasn't\",\n",
    "    \"weren't\",\n",
    "    \"without\",\n",
    "    \"wont\",\n",
    "    \"wouldnt\",\n",
    "    \"won't\",\n",
    "    \"wouldn't\",\n",
    "    \"rarely\",\n",
    "    \"seldom\",\n",
    "    \"despite\",\n",
    "}\n",
    "\n",
    "positive_file = open('positive-words.txt', 'r')\n",
    "POSITIVE = positive_file.read().splitlines()\n",
    "positive_file.close()\n",
    "\n",
    "negative_file = open('negative-words.txt', 'r')\n",
    "NEGATIVE = negative_file.read().splitlines()\n",
    "negative_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_json(r'reviews_Automotive_5.json', lines = True)\n",
    "\n",
    "data_frame.head()\n",
    "\n",
    "data_frame = data_frame.dropna()\n",
    "data_frame = data_frame.drop(['asin', 'helpful', 'reviewTime', 'reviewerID', 'reviewerName', 'summary', 'unixReviewTime'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    neg_words = NEGATE\n",
    "    if any(word.lower() in neg_words for word in input_words):\n",
    "        return True\n",
    "    if include_nt:\n",
    "        if any(\"n't\" in word.lower() for word in input_words):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def negated_word(word, include_nt=True):\n",
    "    neg_words = NEGATE\n",
    "    if word in neg_words:\n",
    "        return True\n",
    "    if include_nt:\n",
    "        if word == \"n't\":\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def get_negation_statistic(data_frame):\n",
    "    #Negation Counter\n",
    "    total_negation_count = 0\n",
    "    total_negation_sentence_count = 0\n",
    "    max_negation_count = 0\n",
    "    #Total sentence counter\n",
    "    total_sentence_count = 0\n",
    "    \n",
    "    #Sentiment Counter\n",
    "    total_sentiment_count = 0\n",
    "    total_negation_sentiment_count = 0\n",
    "    \n",
    "    reviewList = data_frame['reviewText'].tolist()\n",
    "    for sentence in reviewList:\n",
    "        sentence_tokenizer = word_tokenize(sentence)\n",
    "        review_negation_count = 0\n",
    "        review_sentiment_count = 0\n",
    "        for word in sentence_tokenizer:\n",
    "            if negated_word(word):\n",
    "                review_negation_count += 1\n",
    "            if word == '.': #at the end of the sentence\n",
    "                total_sentence_count += 1\n",
    "                if review_negation_count == 0: #There's no negation word\n",
    "                    pass\n",
    "                else: #There's a negation\n",
    "                    if review_negation_count > max_negation_count:\n",
    "                        print(review_negation_count)\n",
    "                        max_negation_count = review_negation_count\n",
    "                    total_negation_count += review_negation_count\n",
    "                    total_negation_sentence_count += 1\n",
    "                    review_negation_count = 0\n",
    "    return total_negation_count, total_negation_sentence_count, total_sentence_count, total_negation_count/total_negation_sentence_count, max_negation_count\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "def get_sentiment(sentence_token):\n",
    "    positive_index = []\n",
    "    negative_index = []\n",
    "#     sentence_token = nltk.word_tokenize(sentence)\n",
    "    for wordIndex in range(len(sentence_token)):\n",
    "        word = sentence_token[wordIndex].lower()\n",
    "        if word in stop_words or word in NEGATE:\n",
    "            pass\n",
    "        else:\n",
    "            if word in NEGATIVE:\n",
    "                  negative_index.append(wordIndex)  \n",
    "            elif word in POSITIVE:\n",
    "                positive_index.append(wordIndex)\n",
    "    return positive_index, negative_index\n",
    "\n",
    "\n",
    "def get_negation(sentence_token):\n",
    "    negation_list = []\n",
    "    pos_after_negation = []\n",
    "#     sentence_token = nltk.word_tokenize(sentence)\n",
    "    for wordIndex in range(len(sentence_token)):\n",
    "        word = sentence_token[wordIndex].lower()\n",
    "        if negated_word(word):\n",
    "            negation_list.append(wordIndex)\n",
    "            \n",
    "    return negation_list, len(negation_list)\n",
    "    \n",
    "def is_closest_sentiment_positive(value, pos_list, neg_list):\n",
    "    smallest = 1000000\n",
    "    positive = 1\n",
    "    for item in pos_list:\n",
    "        if item > value and item < smallest:\n",
    "            smallest = item\n",
    "    for item in neg_list:\n",
    "        if item > value and item < smallest:\n",
    "            positive = 0\n",
    "    if smallest == 1000000:\n",
    "        positive = -1\n",
    "    return positive\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    vector = []\n",
    "    sentence_token = nltk.word_tokenize(sentence)\n",
    "    pos_index , neg_index = get_sentiment(sentence_token)\n",
    "    negate_index, num_negate = get_negation(sentence_token)\n",
    "    pos_score = len(pos_index)\n",
    "    neg_score = len(neg_index)\n",
    "    num_sent = pos_score + neg_score\n",
    "    if num_sent == 0:\n",
    "        return None\n",
    "    else:\n",
    "        vector.append(num_negate)\n",
    "        vector.append(num_sent)\n",
    "        vector.append(pos_score)\n",
    "        vector.append(neg_score)\n",
    "        \n",
    "        for negateIndex in negate_index:\n",
    "            vector.append(negateIndex)\n",
    "            vector.append(is_closest_sentiment_positive(negateIndex, pos_index, neg_index))\n",
    "    \n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This product serves its purpose. I use it for hauling canoes to and from our cabing. If I would have had time I could have made this myself for about $30. It is very sloppy if you don't secure it with ratchet straps on each side back to the truck. Works good though if only going short distances and moderate speeds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 7, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_frame['reviewText'][16])\n",
    "preprocess_sentence(\"It is very sloppy if you don't secure it with ratchet straps on each side back to the truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23263, 19717, 88743, 1.1798448039762641, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_negation_statistic(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
